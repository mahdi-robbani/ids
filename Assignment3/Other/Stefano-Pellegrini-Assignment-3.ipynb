{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1{-}\n",
    "\n",
    "#### 1.2. PC scatterplot of the standardized murders dataset       \n",
    "$\\,$   \n",
    "$\\,$     \n",
    "I decided to normalize the murders dataset because its two features are in totally different units.\n",
    "$\\,$\n",
    "\n",
    "![plot1](plot1.png) \n",
    "    \n",
    "$\\,$  \n",
    "$\\,$  \n",
    "$\\,$  \n",
    "$\\,$   \n",
    "$\\,$   \n",
    "$\\,$  \n",
    "$\\,$  \n",
    "$\\,$  \n",
    "\n",
    "#### 1.3. Variance and PCs (pesticides centered training dataset)\n",
    "\n",
    "\n",
    "   + __Plot of cumulative normalized variance versus PC__  \n",
    "   ![plot4](plot3.png)\n",
    "   \n",
    "   + __Plot of normalized variance versus PC__  \n",
    "   ![plot3](plot3.png)\n",
    "   \n",
    "   \n",
    "   + __Plot of variance versus PC__  \n",
    "   ![plot2](plot2.png)\n",
    "   \n",
    "   \n",
    "   + __The numbers of dimensions needed to capture 90% and 95% in pesticides training dataset__   \n",
    "       * To capture 95% of variance we need 3 PCs  \n",
    "       * To capture 90% of variance we need 2 PCs  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2{-}\n",
    "\n",
    "+ __PCA plot of pesticides centered training dataset__  \n",
    "    I added the two principal eigenvectors in red, they define the space where the data is now projected. \n",
    "    $\\,$   \n",
    "![plot4](plot5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3{-}\n",
    "\n",
    "+ __Description of software used__  \n",
    "\n",
    "    I used my own implementation of the unsupervised k-means clustering algorithm. As requested in the exercise, I initialized the algorithm placing the k centroids at the k first points of the dataset. To evaluate the quality of the clusters I used my implemented loss function that computes the sum of the squared errors among them. Based on the distances between the points and the centroids, I assign each point to the nearest cluster. Then I update the new centroids computing the mean of the new assigned points, and finally, I compute the loss of the new clusters. The algorithm continues this process until the centroids converge, which occurs when the loss converge.\n",
    "\n",
    "\n",
    "+ __Two cluster centers obtained from centered pesticides training dataset__   \n",
    "$\\,$\n",
    "    + $\\underline{The\\,first\\,centroid}$ =   \n",
    "    (1.86626496e+00, 1.87792821e+01, 3.29138120e+02, 1.30606740e+03, 1.06726675e+02, -6.72401222e+02, -3.77579821e+02, -2.66757684e+02, -2.26995889e+02, -1.36750966e+02, -6.75217692e+01, -1.33063846e+01, -1.26669231e+00)  \n",
    "$\\,$\n",
    "\n",
    "    * $\\underline{The\\,second\\,centroid}$ =   \n",
    "    (-1.64175188e+00, -1.65201203e+01, -2.89542556e+02, -1.14894651e+03, -9.38873759e+01, 5.91510850e+02, 3.32156684e+02, 2.34666534e+02, 1.99688113e+02, 1.20299722e+02, 5.93988496e+01, 1.17056165e+01, 1.11430827e+00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra - Clusters prediction and plotting {-}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4{-}\n",
    "\n",
    "#### 4.1. How is probability interpreted differently in the frequentist and Bayesian views? \n",
    "\n",
    "The Bayesian view interprets probability as a degree of belief or a measure of certainty, the alternative frequentist view interprets probability as a frequency.\n",
    "\n",
    "#### 4.2. Cheap, eï¬€icient computers played a major role in making Bayesian methods mainstream. Why? \n",
    "\n",
    "The posterior probability is often unavailable in closed analytic form but itcanbeapproximatedusingMonteCarlomethodsbasedonsampling, orbyvariationalBaeyesian methods. These methods are computationally heavy and require fast computers to process data.\n",
    "\n",
    "#### 4.3. What is the difference between a Bayesian credible interval and a frequentist confidence interval? \n",
    "\n",
    "A frequentist confidence interval is not a probability, it is a statement, based on a large number of repeated experiments, about the frequency of the parameter being in the interval. In this interpretation the data is random and the parameter is fixed. In the Bayesian view, a certain confidence interval represents the probability, based on the posterior distribution, that the parameter value given the observed data is contained in the interval. In this alternative interpretation, the parameter is random and the data is fixed.\n",
    "\n",
    "#### 4.4. How does a maximum likelihood estimate approximate full Bayesian inference? \n",
    "\n",
    "In Bayesian inference is possible to define a loss function that measures the cost of using a point estimate instead of the true parameter value. A maximum likelihood estimate can approximate the full Bayesian inference if we assume a zero-one loss function and uniform prior, so all values of the parameter should be equally likely. This is often a good approximation but in certain cases it can be badly wrong.\n",
    "\n",
    "#### 4.5. When will point estimates be a good approximation of full Bayesian inference? \n",
    "\n",
    "A point estimate can be a good approximation if the distribution of the parameter is a unimodal distribution with one clear peak."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
