{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plot import *\n",
    "s = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.1\n",
    "def plot_iris(data, name):\n",
    "    if \"2D1\" in name:\n",
    "        y = \"Feature 2\"\n",
    "    else:\n",
    "        y = \"Feature 1\"\n",
    "    \n",
    "    plt.scatter(data[data[:,2] == 0][:, 0], data[data[:,2] == 0][:, 1], ec = \"black\", label = \"Class 0\", zorder = 3)\n",
    "    plt.scatter(data[data[:,2] == 1][:, 0], data[data[:,2] == 1][:, 1], ec = \"black\", color = 'red', label = \"Class 1\", zorder = 3)\n",
    "    plot_template(title = \"Plot of \" + name, xlabel = \"Feature 0\", ylabel = y, equal_axis=False, legend= True, save = s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.2\n",
    "def logistic(x):\n",
    "    out = 1/(1 + np.exp(-x))\n",
    "    return out\n",
    "\n",
    "def add_one(X):\n",
    "    row, col = np.shape(X)\n",
    "    one_col = np.ones(row)\n",
    "    X = np.c_[one_col, X]\n",
    "    return X\n",
    "\n",
    "def gradient(X, y, w):\n",
    "    \"\"\"\n",
    "    Returns a vector of partial derivaties\n",
    "    \"\"\"\n",
    "    s = -y.T * (w.T @ X.T) # Transpose to get 1*d @ d*N\n",
    "    theta = logistic(s) # 1*N\n",
    "    c =  -y * X # N*d\n",
    "    grad = c.T @ theta.T # Transpose to d*N @ N*1\n",
    "    return grad\n",
    "\n",
    "def insample_error(X, y, w):\n",
    "    \"\"\"\n",
    "    Returns a single real value which corresponds to the error\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    s = -y.T * (w.T @ X.T) # Transpose to get 1*N x 1*d @ d*N\n",
    "    pyx = np.log(1 + np.exp(s)) # Calculate P(Yn|Xn) likelihood\n",
    "    error = np.sum(pyx)/N # Calculate sum[P(Yn|Xn)]/N\n",
    "    return error\n",
    "\n",
    "def train_log(X, y):\n",
    "    \"\"\"\n",
    "    Perfoms logistic regression training\n",
    "    Takes in X = N*d array and Y = N*1 array\n",
    "    Returns an array of weights w = d*1\n",
    "    \"\"\"\n",
    "    X = add_one(X) # Add intercept column\n",
    "    N, d = np.shape(X)\n",
    "    w = np.reshape(np.random.randn(d), (d, 1)) #initialize random weights\n",
    "    error = insample_error(X, y , w)\n",
    "    learning_rate = 0.01\n",
    "    iteration = 1\n",
    "    convergance = 0\n",
    "    tolerance = 10**-10\n",
    "    \n",
    "    while convergance == 0:\n",
    "        m = gradient(X, y, w)\n",
    "        w_new = w - (learning_rate * m) # update weight\n",
    "        new_error = insample_error(X, y, w_new)\n",
    "        g = np.linalg.norm(m) # convert partial derivate array to single gradient value\n",
    "        iteration += 1\n",
    "        \n",
    "        #check if new error is better\n",
    "        if new_error < error:\n",
    "            w = w_new\n",
    "            error = new_error\n",
    "            learning_rate *= 1.1\n",
    "        else:\n",
    "            learning_rate *=0.9\n",
    "        \n",
    "        #check convergance condition\n",
    "        if g < tolerance:\n",
    "            #print(\"Tolerance reached\")\n",
    "            convergance = 1\n",
    "        elif iteration == 10000:\n",
    "            #print(\"Max iterations\")\n",
    "            convergance = 1\n",
    "    return w\n",
    "    \n",
    "def predict_log(X, w):\n",
    "    X = add_one(X) #Add column for intercept\n",
    "    pred = logistic(w.T @ X.T).T # h(x) = theta(w.T @ x)\n",
    "    pred = pred > 0.5 # Convert prediction to a boolean that indicates if it is > 0.5 or not\n",
    "    pred = np.array(pred, dtype = int) # convert to 0 or 1\n",
    "    pred = 2*(pred-0.5) # conver to -1 or 1\n",
    "    return pred\n",
    "\n",
    "def get_error(true, pred):\n",
    "    \"\"\"\n",
    "    Takes in two N*1 arrays\n",
    "    Each array consists of -1 or 1\n",
    "    Returns a single error value\n",
    "    \"\"\"\n",
    "    N = len(true)\n",
    "    error = abs(true - pred)/2 #convert each value to 0 or 1\n",
    "    error = np.sum(error)/N\n",
    "    return error\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\"\n",
    "    Saves last column as y. Converts y to -1 or 1\n",
    "    Saves rest of the columns as x\n",
    "    \"\"\"\n",
    "    x = data[:, :2]\n",
    "    y = data[:, -1:]\n",
    "    y = 2*(y-0.5) # convert to -1 or 1\n",
    "    return x, y\n",
    "\n",
    "def log_regression(train, test):\n",
    "    \"\"\"\n",
    "    Perform logistic regression on a dataset with y value as last column\n",
    "    Returns the 0-1 error value, weights, and predicted values\n",
    "    \"\"\"\n",
    "    train_x, train_y = split_data(train)\n",
    "    test_x, test_y = split_data(test)\n",
    "    weights = train_log(train_x, train_y)\n",
    "    pred = predict_log(test_x, weights)\n",
    "    error = get_error(test_y, pred)\n",
    "    return error, weights, pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
